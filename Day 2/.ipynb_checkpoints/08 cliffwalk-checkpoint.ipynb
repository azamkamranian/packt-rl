{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Policy optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Montecarlo Policy Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example implementation of REINFORCE, as explained in the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(s,a):\n",
    "    return (2*a-1)*np.tanh(s)\n",
    "\n",
    "def softmax_policy(theta, actions):\n",
    "    def policy_fn(state):\n",
    "        exp = np.exp([np.dot(featurize(state,a),theta) for a in actions])\n",
    "        probs = exp/np.sum(exp)\n",
    "        return probs\n",
    "    return policy_fn\n",
    "\n",
    "def run_episode(env, random_policy): \n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    episode = []\n",
    "    actions = range(env.action_space.n)\n",
    "    while not done:\n",
    "        probs = random_policy(state)\n",
    "        action = np.random.choice(actions, p=probs)\n",
    "        new_state, reward, done, _ = env.step(action)\n",
    "        episode.append((state,action,reward))\n",
    "        state = new_state\n",
    "        total_reward += reward\n",
    "    return episode, total_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = 2*np.random.rand(4)-1\n",
    "n_episodes = 10000\n",
    "actions = range(env.action_space.n)\n",
    "gamma = 1.\n",
    "alpha = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward in episode 1000: 99.0\n",
      "Won! this happened in episode  1244\n"
     ]
    }
   ],
   "source": [
    "for ep in range(n_episodes):\n",
    "    G = 0\n",
    "    policy = softmax_policy(theta,actions)\n",
    "    episode, ep_reward = run_episode(env,policy)\n",
    "    \n",
    "    if ep_reward == 200:\n",
    "        print(\"Won! this happened in episode \", ep)\n",
    "        break\n",
    "    \n",
    "    if (ep+1) % 1000 == 0:\n",
    "        print(\"Reward in episode {}: {}\".format(ep+1,ep_reward))\n",
    "    \n",
    "    \n",
    "    for i, _ in enumerate(episode):\n",
    "        s, a, r = _\n",
    "        G = sum(x[2]*(gamma**j) for j, x in enumerate(episode[i:]))\n",
    "        s = np.array(s)\n",
    "        theta = theta + alpha*(featurize(s,a)-np.sum([policy(s)[a]*featurize(s,a) for a in actions]))*G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn:\n",
    "\n",
    "- Implement REINFORCE and REINFORCE with baseline for `CliffWalking`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional material\n",
    "\n",
    "If you feel ready for a more interesting challenge, you can try `Pong`, which is one of the simples Atari environments. You should do some preprocessing of the video frames, to speed up the computation (resize, downsample, and remove background or color altogether).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to solve this yourself first! If you get stuck or want some inspiration, look at Andrej Karpathy's post:\n",
    "\n",
    "- http://karpathy.github.io/2016/05/31/rl/\n",
    "- https://gist.github.com/karpathy/a4166c7fe253700972fcbc77e4ea32c5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
