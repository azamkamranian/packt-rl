{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# The estimator is of the form (s,a) -> scalar value\n",
    "\n",
    "class FunctionEstimator:\n",
    "    def __init__(self,n_actions):\n",
    "        self.n_actions = n_actions\n",
    "        self.initial_state = env.reset()\n",
    "        self.model = self._build_model()\n",
    "        self.memory_buffer = deque(maxlen=2000)\n",
    "        self.update_buffer = []\n",
    "\n",
    "    def _concat(self, state, action):\n",
    "        return np.hstack([state,action]).reshape(1,-1)\n",
    "\n",
    "    def _build_model(self):\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=(80*80+1), activation='tanh'))\n",
    "        model.add(Dense(24, activation='tanh'))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        model.compile(\n",
    "                    loss='mse'\n",
    "                    , optimizer= Adam(lr = 0.001)\n",
    "                    )      \n",
    "        return model\n",
    "    \n",
    "    def update(self,buffer):\n",
    "        states = [buffer[ix][0] for ix in range(len(buffer))]\n",
    "        actions = [buffer[ix][1] for ix in range(len(buffer))]\n",
    "        td_targets = [buffer[ix][2] for ix in range(len(buffer))]\n",
    "        for state, action, target in zip(states, actions, td_targets):\n",
    "            self.model.fit(self._concat(state,action), [td_target], verbose=0)\n",
    "        \n",
    "    def predict(self,state):\n",
    "        concats = [np.array([self._concat(state, a)]).reshape(1,-1) for a in range(self.n_actions)]\n",
    "        return [self.model.predict(c) for c in concats]\n",
    "\n",
    "    \n",
    "    def remember(self, state, action,td_target):\n",
    "        self.memory_buffer.append((state,action,td_target))\n",
    "    \n",
    "    \n",
    "    def replay(self,batch_size):\n",
    "        # Experience replay\n",
    "            # choose only a sample from the collected experience\n",
    "        update_buffer_idxs = np.random.choice(len(self.memory_buffer)\n",
    "                                                , size=min(len(self.memory_buffer), batch_size)\n",
    "                                                , replace=False\n",
    "                                                    ) \n",
    "        update_buffer_idxs = np.ravel(update_buffer_idxs)\n",
    "                \n",
    "        for ix in range(len(update_buffer_idxs)):\n",
    "            saved_ix = update_buffer_idxs[ix]\n",
    "            self.update_buffer.append(self.memory_buffer[saved_ix])\n",
    "            \n",
    "        self.update(self.update_buffer)\n",
    "\n",
    "\n",
    "## Auxiliary function for the policy\n",
    "def make_policy(estimator, n_actions, ep):\n",
    "    def policy_fn(state):\n",
    "        preds = np.ravel(estimator.predict(state))\n",
    "        noise = np.ravel(np.random.randn(1,n_actions)*(1./(ep+1)))\n",
    "        action = np.argmax(preds+noise)\n",
    "        return action\n",
    "    return policy_fn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(I):\n",
    "    \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n",
    "    I= I[35:195] # crop\n",
    "    I = I[::2,::2,0] # downsample by factor of 2\n",
    "    I[I == 144] = 0 # erase background (background type 1)\n",
    "    I[I == 109] = 0 # erase background (background type 2)\n",
    "    I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n",
    "    return I.astype(np.float).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "env = gym.make('Pong-v0')\n",
    "n_episodes = 1000\n",
    "gamma = 1\n",
    "estimator = FunctionEstimator(env.action_space.n)\n",
    "score = []\n",
    "\n",
    "for ep in range(n_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    policy = make_policy(estimator, env.action_space.n, ep)\n",
    "    ep_reward = 0\n",
    "    while not done:\n",
    "        state = preprocess(obs)\n",
    "        action = policy(state)\n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "        new_state = preprocess(new_obs)\n",
    "        ep_reward += reward\n",
    "        # Update the Q-function\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            td_target = reward + gamma*np.argmax(estimator.predict(new_state))\n",
    "        \n",
    "        estimator.remember(state,action, td_target)\n",
    "        # Update the state\n",
    "        state = new_state\n",
    "        #\n",
    "    estimator.replay(32)\n",
    "    # Show stats\n",
    "    if done:\n",
    "        if len(score) < 100:\n",
    "            score.append(ep_reward)\n",
    "        else:\n",
    "            score[ep % 100] = ep_reward\n",
    "    if (ep+1) % 100 == 0:\n",
    "        estimator.model.save_weights('./out/pong-keras-{}.h5'.format(ep+1))\n",
    "        print(\"Number of episodes: {} . Average 100-episode reward: {}\".format(ep+1, np.mean(score)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(obs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To plot pretty figures and animations\n",
    "%matplotlib nbagg\n",
    "import matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    plt.close()  # or else nbagg sometimes plots in the previous cell\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    return animation.FuncAnimation(fig, update_scene, fargs=(frames, patch), frames=len(frames), repeat=repeat, interval=interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "estimator = FunctionEstimator(env.action_space.n)\n",
    "estimator.model.load_weights('./out/pong-keras-100.h5')\n",
    "done = False\n",
    "obs = env.reset()\n",
    "policy = make_policy(estimator, env.action_space.n, 1)\n",
    "while not done:\n",
    "    state = preprocess(obs)\n",
    "    action = policy(state)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    \n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    frames.append(img)\n",
    "    \n",
    "plot_animation(frames)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
