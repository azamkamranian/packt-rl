{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL as a blackbox optimization problem\n",
    "\n",
    "Instead of looking at the value functions $Q$ and $V$ we can look directly at the policy. We assume our policy is parameterized by some vector $\\theta$ and try to optimize the reward by changing the values of $\\theta$.\n",
    "\n",
    "If you have some background in calculus, your instinct would tell you to use the gradient of the reward and update the parameter in the direction of this gradient!\n",
    "\n",
    "Gradients, however, are usually bad news for reinforcement learning. One issue is rewards that are far in the future, but perhaps the most serious downside is that the total reward function is very sensitive to the parameter. Imagine that you had a situation were you want to train a model, for instance, linear regression, on your dataset, and that the data would change every time you change the parameter! This makes the situation rather tricky.\n",
    "\n",
    "Luckily, there is a large class of optimization methods that do not use of the gradients and some of those are easy to parallelize. We will discuss two methods which have shown promising results. Other variations of heuristic methods (for instance, genetic algorithms) could also be applied.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Entropy Method\n",
    "\n",
    "* Blackbox optimization for $f(w)$.\n",
    "* Start with $\\mu, \\sigma$, `batch_size`, `elite_frac`.\n",
    "* Do forever:\n",
    "  * Sample `batch_size` possible values for $w$ from the normal distribution with mean $\\mu$ and $\\sigma$.\n",
    "  * Evaluate $f$ on the sampled values.\n",
    "  * Keep the top `elite_frac` of those. These are `elite_set`.\n",
    "  * $\\mu \\leftarrow$ `np.mean(elite_set)`\n",
    "  * $\\sigma \\leftarrow$ `np.std(elite_set)`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Evolution Strategies\n",
    "\n",
    "It might not be good idea to take some elite solutions to resample and throw away the rest, since \"weak\" parameters are valuable because they tell us what *not* to do. An alternative approach from CEM is to maximise the expected value of the reward, so that when we would sample from that population, we would very likely get lucky and pick a good parameter.    \n",
    "\n",
    "[It has been shown](https://blog.openai.com/evolution-strategies/) that NES is comparable results to fancier algorithms in a number of tasks\n",
    "\n",
    "\n",
    "\n",
    "### NES Algorithm\n",
    "\n",
    "* Start with $\\sigma$, $\\alpha$, `n_estimators`.\n",
    "* Sample $\\epsilon_1, \\epsilon_2$, ... from a normal distribution with mean $0$ and variance $1$. These are **vectors**.\n",
    "* Calculate return $f(w+\\sigma \\cdot \\epsilon_i)$.\n",
    "*  $w \\leftarrow w + \\alpha\\cdot \\frac{1}{N}\\sum_{i=1}^n\\left(\\frac{f(w+\\sigma\\epsilon_i)-f(w)}{\\sigma}\\epsilon_i\\right)$ \n",
    "* Stopping criteria: function changes below a threshold, or, in the case of OpenAI Gym, performance benchmark is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "(array([ 0.03004524,  0.19704072,  0.03771322, -0.27135001]), 1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "print(env.step(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "iter 5. w: [0.09800009 0.44027144 0.19823706 0.05913848], reward: 32.280000\n",
      "iter 10. w: [0.09745052 0.44949785 0.18343849 0.02247205], reward: 33.545000\n",
      "iter 15. w: [ 0.09847893  0.45501233  0.16510913 -0.01404753], reward: 36.325000\n",
      "iter 20. w: [ 0.10696695  0.45605626  0.1435807  -0.04767888], reward: 40.570000\n",
      "iter 25. w: [ 0.11175131  0.45348567  0.1219771  -0.08063151], reward: 45.180000\n"
     ]
    }
   ],
   "source": [
    "## Code sample: NES for FrozenLake\n",
    "import numpy as np\n",
    "import gym\n",
    "np.random.seed(0)\n",
    "\n",
    "env= gym.make('CartPole-v0')\n",
    "\n",
    "# the function we want to optimize\n",
    "def f(w):\n",
    "    n_iter = 200\n",
    "    total_reward = 0\n",
    "    for it in range(n_iter):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = 1 if np.matmul(state,w) < 0 else 0\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            total_reward+=reward\n",
    "            state = new_state\n",
    "    return total_reward/(1+it)\n",
    "\n",
    "# hyperparameters\n",
    "npop = 100 # population size\n",
    "sigma = 0.1 # noise standard deviation\n",
    "alpha = 0.001 # learning rate\n",
    "\n",
    "w = 2*np.random.random(4)-1 # our initial guess is random\n",
    "\n",
    "i = 0\n",
    "\n",
    "while f(w) <50:\n",
    "    i+=1\n",
    "  # print current fitness of the most likely parameter setting\n",
    "    if i % 5 == 0:\n",
    "        print('iter %d. w: %s, reward: %f' %(i, str(w),  f(w)))\n",
    "\n",
    "  # initialize memory for a population of w's, and their rewards\n",
    "    N = np.random.randn(npop, 4) # samples from a normal distribution N(0,1)\n",
    "    R = np.zeros(npop)\n",
    "    for j in range(npop):\n",
    "        w_try = w + sigma*N[j] # jitter jw using gaussian of sigma 0.1\n",
    "        R[j] = f(w_try) # evaluate the jittered version\n",
    "    \n",
    "    # standardize the rewards to have a gaussian distribution\n",
    "    A = (R - np.mean(R)) / np.std(R)\n",
    "    # perform the parameter update. The matrix multiply below\n",
    "    # is just an efficient way to sum up all the rows of the noise matrix N,\n",
    "    # where each row N[j] is weighted by A[j]\n",
    "    w = w + alpha/(npop*sigma+1) * np.dot(N.T, A)\n",
    "  \n",
    "env.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Reward in iteration 1: 62.0\n",
      "Reward in iteration 2: 37.0\n",
      "Reward in iteration 3: 75.0\n",
      "Reward in iteration 4: 200.0\n",
      "Reward in iteration 5: 200.0\n",
      "Reward in iteration 6: 200.0\n",
      "Reward in iteration 7: 200.0\n",
      "Reward in iteration 8: 200.0\n",
      "Reward in iteration 9: 200.0\n",
      "Reward in iteration 10: 200.0\n",
      "Reward in iteration 11: 200.0\n",
      "Reward in iteration 12: 200.0\n",
      "Reward in iteration 13: 200.0\n",
      "Reward in iteration 14: 200.0\n",
      "Reward in iteration 15: 200.0\n",
      "Reward in iteration 16: 200.0\n",
      "Reward in iteration 17: 200.0\n",
      "Reward in iteration 18: 200.0\n",
      "Reward in iteration 19: 200.0\n",
      "Reward in iteration 20: 200.0\n",
      "Reward in iteration 21: 200.0\n",
      "Reward in iteration 22: 200.0\n",
      "Reward in iteration 23: 200.0\n",
      "Reward in iteration 24: 200.0\n",
      "Reward in iteration 25: 200.0\n",
      "Reward in iteration 26: 200.0\n",
      "Reward in iteration 27: 200.0\n",
      "Reward in iteration 28: 200.0\n",
      "Reward in iteration 29: 200.0\n",
      "Reward in iteration 30: 200.0\n",
      "Reward in iteration 31: 200.0\n",
      "Reward in iteration 32: 200.0\n",
      "Reward in iteration 33: 200.0\n",
      "Reward in iteration 34: 200.0\n",
      "Reward in iteration 35: 200.0\n",
      "Reward in iteration 36: 200.0\n",
      "Reward in iteration 37: 200.0\n",
      "Reward in iteration 38: 200.0\n",
      "Reward in iteration 39: 200.0\n",
      "Reward in iteration 40: 200.0\n",
      "Reward in iteration 41: 200.0\n",
      "Reward in iteration 42: 200.0\n",
      "Reward in iteration 43: 200.0\n",
      "Reward in iteration 44: 200.0\n",
      "Reward in iteration 45: 200.0\n",
      "Reward in iteration 46: 200.0\n",
      "Reward in iteration 47: 200.0\n",
      "Reward in iteration 48: 200.0\n",
      "Reward in iteration 49: 200.0\n",
      "Reward in iteration 50: 200.0\n",
      "Reward in iteration 51: 200.0\n",
      "Reward in iteration 52: 200.0\n",
      "Reward in iteration 53: 200.0\n",
      "Reward in iteration 54: 200.0\n",
      "Reward in iteration 55: 200.0\n",
      "Reward in iteration 56: 200.0\n",
      "Reward in iteration 57: 200.0\n",
      "Reward in iteration 58: 200.0\n",
      "Reward in iteration 59: 200.0\n",
      "Reward in iteration 60: 200.0\n",
      "Reward in iteration 61: 200.0\n",
      "Reward in iteration 62: 200.0\n",
      "Reward in iteration 63: 200.0\n",
      "Reward in iteration 64: 200.0\n",
      "Reward in iteration 65: 200.0\n",
      "Reward in iteration 66: 200.0\n",
      "Reward in iteration 67: 200.0\n",
      "Reward in iteration 68: 200.0\n",
      "Reward in iteration 69: 200.0\n",
      "Reward in iteration 70: 200.0\n",
      "Reward in iteration 71: 200.0\n",
      "Reward in iteration 72: 200.0\n",
      "Reward in iteration 73: 200.0\n",
      "Reward in iteration 74: 200.0\n",
      "Reward in iteration 75: 200.0\n",
      "Reward in iteration 76: 200.0\n",
      "Reward in iteration 77: 200.0\n",
      "Reward in iteration 78: 200.0\n",
      "Reward in iteration 79: 200.0\n",
      "Reward in iteration 80: 200.0\n",
      "Reward in iteration 81: 200.0\n",
      "Reward in iteration 82: 200.0\n",
      "Reward in iteration 83: 200.0\n",
      "Reward in iteration 84: 200.0\n",
      "Reward in iteration 85: 200.0\n",
      "Reward in iteration 86: 200.0\n",
      "Reward in iteration 87: 200.0\n",
      "Reward in iteration 88: 200.0\n",
      "Reward in iteration 89: 200.0\n",
      "Reward in iteration 90: 200.0\n",
      "Reward in iteration 91: 200.0\n",
      "Reward in iteration 92: 200.0\n",
      "Reward in iteration 93: 200.0\n",
      "Reward in iteration 94: 200.0\n",
      "Reward in iteration 95: 200.0\n",
      "Reward in iteration 96: 200.0\n",
      "Reward in iteration 97: 200.0\n",
      "Reward in iteration 98: 200.0\n",
      "Reward in iteration 99: 200.0\n",
      "Reward in iteration 100: 200.0\n"
     ]
    }
   ],
   "source": [
    "## Code sample: Cross entropy method for CartPole\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "# Task settings:\n",
    "env = gym.make('CartPole-v0') # Change as needed\n",
    "num_steps = 500 # maximum length of episode\n",
    "# Alg settings:\n",
    "n_iter = 100 # number of iterations of CEM\n",
    "batch_size = 25 # number of samples per batch\n",
    "elite_frac = 0.2 # fraction of samples used as elite set\n",
    "\n",
    "\n",
    "# Initialize mean and standard deviation\n",
    "dim_theta = (env.observation_space.shape[0]+1) * env.action_space.n\n",
    "theta_mean = np.zeros(dim_theta)\n",
    "theta_std = np.ones(dim_theta)\n",
    "\n",
    "\n",
    "def make_policy(theta):\n",
    "    n_states = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "    W = theta[0 : n_states * n_actions].reshape(n_states, n_actions)\n",
    "    b = theta[n_states * n_actions : None].reshape(1, n_actions)\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        y = state.dot(W) + b\n",
    "        action = y.argmax()\n",
    "        return action\n",
    "        \n",
    "    return policy_fn\n",
    "\n",
    "def run_episode(theta, num_steps, render=False):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    policy = make_policy(theta)\n",
    "    for t in range(num_steps):\n",
    "        a = policy(state)\n",
    "        state, reward, done, _ = env.step(a)\n",
    "        total_reward += reward\n",
    "        if render and t%3==0: env.render()\n",
    "        if done: break\n",
    "    return total_reward\n",
    "\n",
    "def noisy_evaluation(theta):\n",
    "    total_reward = run_episode(theta, num_steps)\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Now, for the algorithms\n",
    "for it in range(n_iter):\n",
    "    # Sample parameter vectors\n",
    "    thetas = np.random.normal(theta_mean, theta_std, (batch_size,dim_theta))\n",
    "    rewards = [noisy_evaluation(theta) for theta in thetas]\n",
    "    # Get elite parameters\n",
    "    n_elite = int(batch_size * elite_frac)\n",
    "    elite_inds = np.argsort(rewards)[batch_size - n_elite:batch_size]\n",
    "    elite_thetas = [thetas[i] for i in elite_inds]\n",
    "    # Update theta_mean, theta_std\n",
    "    theta_mean = np.mean(elite_thetas,axis=0)\n",
    "    theta_std = np.std(elite_thetas,axis=0)\n",
    "    #print(\"Mean reward f: {}. \\\n",
    "    # Max reward: {}\".format(np.mean(rewards), np.max(rewards)))\n",
    "    tot_reward = run_episode(theta_mean, num_steps, render=False)\n",
    "    print(\"Reward in iteration {}: {}\".format(it+1,tot_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CEM in steroids: DeepCEM\n",
    "\n",
    "Instead of generating the samples from a normal distribution, we can use an arbitrary (and unknown) distribution approximated by neural networks! Each step of the optimization generates a sample produced by a neural network and adjusts its weights based on the outcome. Example from [Yandex Data School](https://github.com/yandexdataschool/Practical_RL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning:** It can take a long time to converge! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pablo\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:44<00:00,  2.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a682071588>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class DeepCEM():\n",
    "    def __init__(self, initial_state, \\\n",
    "                 n_actions, \\\n",
    "                 clf=MLPClassifier(hidden_layer_sizes=(4,2), \\\n",
    "                                   activation='tanh', \\\n",
    "                                  warm_start=True, #keep progress between .fit(...) calls\n",
    "                                  max_iter=1 #make only 1 iteration on each .fit(...)\n",
    "                             )):\n",
    "        self.clf = clf\n",
    "        \n",
    "        #initialize clf\n",
    "        clf.fit([initial_state]*n_actions,range(n_actions))\n",
    "        \n",
    "    def policy(self, state):\n",
    "        probs = self.clf.predict_proba([state])[0]\n",
    "        action = np.random.choice(n_actions, p=probs)\n",
    "        return action\n",
    "\n",
    "    def train(self,n_iter):\n",
    "        n_episodes = 100 \n",
    "        percentile = 75\n",
    "        \n",
    "        for i in tqdm(range(n_iter)):\n",
    "            #generate new episodes\n",
    "            episodes = [play_episode(self) \\\n",
    "                        for _ in range(n_episodes)]\n",
    "\n",
    "            batch_states, batch_actions, batch_rewards = \\\n",
    "            map(np.array,zip(*episodes))\n",
    "\n",
    "            reward_threshold = np.percentile(batch_rewards,70)\n",
    "            idxs = [i for \\\n",
    "                    i in range(len(batch_rewards)) \\\n",
    "                    if batch_rewards[i]>reward_threshold]\n",
    "\n",
    "            elite_states, elite_actions = \\\n",
    "            np.concatenate(batch_states[idxs],axis=0), \\\n",
    "            np.concatenate(batch_actions[idxs],axis=0)\n",
    "            self.clf.fit(elite_states, elite_actions)\n",
    "\n",
    "            if np.mean(batch_rewards)> 195:\n",
    "                print(\"Solved.\")\n",
    "\n",
    "\n",
    "def play_episode(agent, max_iter=1000,render=False):\n",
    "    states,actions = [],[]\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    for _ in range(max_iter):\n",
    "        # choose the action according to the policy\n",
    "        action = agent.policy(state)\n",
    "        new_state,reward,done,info = env.step(action)\n",
    "        if render:\n",
    "            env.render()\n",
    "        # record sessions\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        total_reward+=reward\n",
    "        state = new_state\n",
    "        if done: \n",
    "            break\n",
    "    return states,actions,total_reward\n",
    "\n",
    "env = gym.make(\"CartPole-v0\").env  \n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "agent = DeepCEM(initial_state=env.reset(), n_actions=2)\n",
    "agent.train(n_iter=100)\n",
    "states, actions, rewards= play_episode(agent, render=False)\n",
    "\n",
    "plt.plot(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
